<!DOCTYPE html>
<html lang="en"><head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-209910972-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-209910972-1');
</script>

	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Pardis Pashakhanloo</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Pardis Pashakhanloo | INTRODUCTION. </title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="Pardis Pashakhanloo">
<meta property="og:locale" content="en_US">
<meta name="description" content="Pardis is a PhD candidate in the Department of Computer and Information Science at the University of Pennsylvania. She specializes in bringing the power of deep learning, databases, and software debloating into making code bases secure, efficient, and defect-free.">
<meta property="og:description" content="Pardis is a PhD candidate in the Department of Computer and Information Science at the University of Pennsylvania. She specializes in bringing the power of deep learning, databases, and software debloating into making code bases secure, efficient, and defect-free.">
<link rel="canonical" href="https://pardisp.github.io/">
<meta property="og:url" content="https://pardisp.github.io/">
<meta property="og:site_name" content="Pardis Pashakhanloo">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Pardis Pashakhanloo">
<script type="application/ld+json">
{"description":"Pardis is a PhD candidate in the Department of Computer and Information Science at the University of Pennsylvania. She specializes in bringing the power of deep learning, databases, and software debloating into making code bases secure, efficient, and defect-free.","url":"https://pardisp.github.io/","@type":"WebSite","headline":"Pardis Pashakhanloo","name":"Pardis Pashakhanloo","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="./_images/main.css">
  <link rel="icon" type="image/png" href="_files/favicon.png"><link type="application/atom+xml" rel="alternate" href="https://pardisp.github.io/feed.xml" title="Pardis Pashakhanloo"><style>@font-face {
	font-family: "wticons";
	src: url("data:font/woff2;charset=utf-8;base64,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") format("woff2");
}

.wticons {
	line-height: 1;
}

.wticons:before {
	font-family: wticons !important;
	font-style: normal;
	font-weight: normal !important;
	vertical-align: top;
}

.wticon-account:before {
	content: "\f101";
}
.wticon-add:before {
	content: "\f102";
}
.wticon-cardResizeDrag:before {
	content: "\f103";
}
.wticon-casual:before {
	content: "\f104";
}
.wticon-check:before {
	content: "\f105";
}
.wticon-checkSmall:before {
	content: "\f106";
}
.wticon-chevron:before {
	content: "\f107";
}
.wticon-copy:before {
	content: "\f108";
}
.wticon-copySmall:before {
	content: "\f109";
}
.wticon-dismiss:before {
	content: "\f10a";
}
.wticon-downChevron:before {
	content: "\f10b";
}
.wticon-error:before {
	content: "\f10c";
}
.wticon-expand:before {
	content: "\f10d";
}
.wticon-feedback:before {
	content: "\f10e";
}
.wticon-filledDownArrow:before {
	content: "\f10f";
}
.wticon-find:before {
	content: "\f110";
}
.wticon-formal:before {
	content: "\f111";
}
.wticon-gift:before {
	content: "\f112";
}
.wticon-grayLogo:before {
	content: "\f113";
}
.wticon-ignore:before {
	content: "\f114";
}
.wticon-info:before {
	content: "\f115";
}
.wticon-leftChevron:before {
	content: "\f116";
}
.wticon-logo:before {
	content: "\f117";
}
.wticon-love:before {
	content: "\f118";
}
.wticon-noRecommendations:before {
	content: "\f119";
}
.wticon-paste:before {
	content: "\f11a";
}
.wticon-pin:before {
	content: "\f11b";
}
.wticon-premium:before {
	content: "\f11c";
}
.wticon-premiumDetail:before {
	content: "\f11d";
}
.wticon-premiumFull:before {
	content: "\f11e";
}
.wticon-recommendationLight:before {
	content: "\f11f";
}
.wticon-recommendationLightCard:before {
	content: "\f120";
}
.wticon-recommendationLightNoSuggestions:before {
	content: "\f121";
}
.wticon-refine:before {
	content: "\f122";
}
.wticon-rewrite:before {
	content: "\f123";
}
.wticon-rightChevron:before {
	content: "\f124";
}
.wticon-rocket:before {
	content: "\f125";
}
.wticon-sentenceExamples:before {
	content: "\f126";
}
.wticon-settings:before {
	content: "\f127";
}
.wticon-shorten:before {
	content: "\f128";
}
.wticon-tutorial:before {
	content: "\f129";
}
.wticon-unlock:before {
	content: "\f12a";
}
.wticon-warn:before {
	content: "\f12b";
}
.wticon-WordtuneButton:before {
	content: "\f12c";
}
.wticon-x:before {
	content: "\f12d";
}

a{
	color: #ad586f;
}
a:hover{
	color: #6c0c39;
}
a:visited {
  color: #6c0c39;
}
</style></head>
<body class="">

    

    <main class="page-content " aria-label="Content">
      <div class="wrapper">
        <div class="home"><header class="site-header" role="banner">

  <div class="wrapper">
    <a class="site-title" rel="author" href="https://pardisp.github.io/"> </a>

      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger">
					<a class="page-link" href="index.html">Home</a>
          <a class="page-link" href="index.html#highlights">Publications</a>
          <a class="page-link" href="index.html#projects">Projects</a>
          <a class="page-link" href="index.html#experience">Experience</a>
          <a class="page-link" href="blog.html">Blog</a>
        </div>
      </nav>
  </div>
</header>
        

<div class="body">
<h1>Random Notes</h1>
<details>
<summary><large>Fine-Tuning a Language Model for Legal Question Answering <small style="font-weight: lighter;"> March, 2024</small></large></summary>
<br>
<p style="font-weight: 200">
The legal domain is filled with complex terminology, nuanced interpretations, and the need for precise answers. This makes it a perfect challenge for the power of natural language processing (NLP). However, generic language models often struggle to grasp the subtleties of legal text. To satisfy my curiosity, in this blog post, I will dive into the process of fine-tuning a large language model (LLM) specifically for the task of legal question answering. You can find <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/raw_data_sample.json">the raw dataset</a> and the <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/ExperimentingWithLegalBench.ipynb">complete notebook</a> in <a href="https://github.com/ppashakhanloo/legal-gemma">legal-gemma</a> repository.
</p>

<h2>The Toolkit</h2>
<ul>
	<li><strong><a href="https://colab.research.google.com/">Google Colab</a>:</strong> The experimentation playground for GPU resources.</li>
	<li><strong><a href="https://hazyresearch.stanford.edu/legalbench/">LegalBench</a>:</strong> A curated dataset of legal questions and answers.</li>
	<li><strong><a href="https://huggingface.co/docs/transformers/index">HuggingFace</a>:</strong> Its libraries streamline working with LLMs.</li>
	<li><strong><a href="https://pypi.org/project/bitsandbytes/">BitsAndBytes</a>:</strong> For memory-efficient quantization.</li>
	<li><strong><a href="https://pypi.org/project/peft/">PEFT</a> and <a href="https://huggingface.co/docs/peft/en/package_reference/lora">LORA</a>:</strong> Techniques to selectively fine-tune parts of the model.</li>
</ul>

<h2>The Dataset: LegalBench</h2>
Before diving into code, let's understand the structure of <a href="https://github.com/HazyResearch/legalbench">LegalBench</a>. According to the owners of the repository, LegalBench is "a benchmark consisting of different legal reasoning tasks. Each task has an associated dataset, consisting of input-output pairs."

<h2>Step-by-Step Guide</h2>
<h3>Project Setup</h3>
<p>First, head to <a href="https://colab.research.google.com/">Google Colab</a> and start a new notebook. Next, make sure you have all the following installed on the notebook:
<pre><code>
!pip install --upgrade pandas datasets transformers bitsandbytes peft trl huggingface_hub
</code></pre>
</p>

<h3>Load and Transform the Data</h3>
<p>I do not use the LegalBench dataset in the originally provided format but instead transform it from a multi-task, multi-dataset form to a question/answering dataset where the answers could be Yes/No, multiple-choice, or open-ended. You can find the [long and ugly] transformation code <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/Data.py">here</a>. Basically, it takes one row from the original dataset and prepares a <it>context</it>, a <it>question</it>, and <it>answer</it> depending on the task type. I decided to move the data transformation code to a separate Python file to improve code readability.</p>

<p>So, in the implementation, I start by reading the provided dataset and transforming it using the data transformation that I built.</p>
<pre><code>
  from Data import transform_data
  import pandas as pd

  df = pd.read_json('raw_data_sample.json')

  data = [transform_data(row) for row in df.itertuples()]
  indexes, contexts, questions, answers = zip(*data)

  assert len(contexts) == len(questions) == len(answers) and len(contexts) > 0
  input_texts = [{"text": f"Answer the Question based on the given Context.\nContext: {c}\nQuestion: {q}\nAnswer: {a}"} for c, q, a in zip(contexts, questions, answers)]
</code></pre>


<p>Next, I use scikit to split the data into two sets: train (95%), and test (5%).</p>
<pre><code>
  from sklearn.model_selection import train_test_split
  from datasets import Dataset

  train_texts, test_texts = train_test_split(input_texts, test_size=0.05, random_state=7)

  train_dataset = Dataset.from_list(train_texts)
  test_dataset = Dataset.from_list(test_texts)
</code></pre>

<h3>Prepare for Loading the Language Model</h3>
<p>Then, I import the necessary libraries for fine-tuning gemma-2b model. I use huggingface for this purpose.</p>
<pre><code>
  import torch

  from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
  from transformers import BitsAndBytesConfig, set_seed

  from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model

  from trl import SFTTrainer

  set_seed(7)
  model_name = "google/gemma-2b-it"
</code></pre>

<p>Since loading <code>gemma-2b</code> requires accepting particular terms and conditions, one needs to accept these terms and login into huggingface hub via an access token. These <a href="https://huggingface.co/google/gemma-2b">terms and conditions</a> can be accepted by logging into <a href=https://huggingface.co>huggingface</a>. Then, login using your access token.</p>
<pre><code>
  from huggingface_hub import login
  login(token='YOUR_TOKEN')
</code></pre>

<h3>Fine-tuning the Model</h3>
<p>The next step is loading the model and the corresponding tokenizer.</p>

<h4>Fitting the LLM on my GPU!</h4>
<p>Since fine-tuning gemma-2b required more memory than I had available (15GB), I needed to take advantage of quantization to reduce the memory usage. Basically, <code>BitsAndBytes</code> quantization uses lower-precision data types to enable loading larger models.</p>

<p>My configuration loads the linear layers of the model with 4-bit integer precision.</p>
<pre><code>
  bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
  )
</code></pre>

<h3>Loading the Model</h3>
<p>I load the model and its corresponding tokenizer using <code>huggingface</code>. Then, I pass it to <code>prepare_model_for_kbit_training</code> so that the <code>BitsAndBytes</code> quantization that I configured earlier is applied to the loaded model.</p>
<pre><code>
  tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True, truncation_side = "left")
  tokenizer.pad_token = tokenizer.eos_token
  tokenizer.padding_value = tokenizer.eos_token_id
  model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)
  model = prepare_model_for_kbit_training(model)
</code></pre>

<p>The next configuration that I found necessary for successfully fine-tuning <code>gemma-2b</code> within my time and hardware constraint was reducing the number of trainable parameters. <a href="https://arxiv.org/abs/2106.09685">Recent work</a> has shown that even training a very small percentage of pretrained parameters can be beneficial. That is where <code>LoraConfig</code> comes in.
LoRa technique freezes values of all the parameters in the pretrained model and introduces a pair of matrices that can be trained into each layer of the Transformer to be trained instead.</p>

<pre><code>
  peft_config = LoraConfig(
    r=8,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=['q_proj', 'up_proj', 'v_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj']
  )
	model = get_peft_model(model, peft_config)
</code></pre>
<p>Then, I set the training arguments. Due to the time constraints, I experimented with only 3 different learning rates to choose the best one, and kept the rest of the hyperparameters fixed. Since the fine-tuning data is relatively small, I decided to only do a small number of training steps.</p>

<pre><code>
  training_arguments = TrainingArguments(
    output_dir="./results",
    do_eval=True,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    optim="paged_adamw_8bit",
    save_steps=100,
    logging_steps=100,
    learning_rate=1e-5,
    eval_steps=100,
    num_train_epochs=1,
    warmup_ratio=0.02,
    lr_scheduler_type="linear",
    load_best_model_at_end=True,
    save_strategy="steps",
    evaluation_strategy="steps"
  )
</code></pre>

<p>Next, I set up the fine-tuning trainer so that it uses the <code>training_arguments</code> defined above.</p>
<pre><code>
  trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    dataset_text_field="text",
    tokenizer=tokenizer,
    args=training_arguments,
    peft_config=peft_config,
    max_seq_length=512
  )
</code></pre>

<h3>Measuring the Preplexity before Fine-tuning</h3>
<p>Before fine-tuning, I will measure the preplexity of the original (non-fine-tuned) model on the legal test set.
</p>

<pre><code>
  import math

  eval_results = trainer.evaluate()
  print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
</code></pre>

<pre><samp>
  Perplexity: 67.86
</samp></pre>

<p>Let's keep the model's answers to 100 randomly chosen samples from the test set. I will use them to manually evaluate the models' responses.</p>
<pre><code>
  from random import choices

  subset_test_dataset = list(test_dataset.select(choices(range(len(test_dataset)), k=100)))

  with open('ground_truth_subset_test.txt', 'w') as f:
    for i, entry in enumerate(subset_test_dataset):
      f.write(str(i) + "\n")
      f.write(entry['text'])
      f.write('\n====================\n')

  with open('original_subset_test.txt', 'w') as f:
    for i, entry in enumerate(subset_test_dataset):
      text = entry["text"][:entry["text"].find('\nAnswer: ')] + '\nAnswer: '
      inputs = tokenizer(text, return_tensors="pt", add_special_tokens=True)
      outputs = model.generate(**inputs, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)
      result = tokenizer.decode(outputs[0], skip_special_tokens=True)
      f.write(str(i) + "\n")
      f.write(result)
      f.write('\n====================\n')
</code></pre>

<h3>Fine-tuning</h3>
<p>Now, we can finally start the fine-tuning using the configurations and hyperparameters set earlier.
</p>
<pre><code>
  torch.cuda.empty_cache()
  model.config.use_cache = False
  trainer.train()
  model.config.use_cache = True
</code></pre>

<h3>Save the Model</h3>
<pre><code>
  trainer.model.save_pretrained('legal_'+model_name)
</code></pre>

<h3>Evaluating the Preplexity of the New Model</h3>
<p>Let's evaluate the preplexity of the fine-tuned model.</p>
<pre><code>
  new_eval_results = trainer.evaluate()
  print(f"Perplexity: {math.exp(new_eval_results['eval_loss']):.2f}")
  assert new_eval_results['eval_loss'] < eval_results['eval_loss']
</code></pre>

Output:
<pre><samp>
	Perplexity: 4.56
</samp></pre>

<p>As expected (and desired!), the preplexity of the fine-tuned model is lower than that of the original model.</p>

<h3>Manual Evaluation</h3>
<p>Let's record the model's answers to a subset of the test set.</p>
<pre><code>
  with open('fine-tuned_subset_test.txt', 'w') as f:
    for i, entry in enumerate(subset_test_dataset):
      text = entry["text"][:entry["text"].find('\nAnswer: ')] + '\nAnswer: '
      inputs = tokenizer(text, return_tensors="pt", add_special_tokens=True)
      outputs = model.generate(**inputs, max_new_tokens=20)
      result = tokenizer.decode(outputs[0], skip_special_tokens=True)
      f.write(str(i) + "\n")
      f.write(result)
      f.write('\n====================\n')
</code></pre>

<p> I used <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/ground_truth_subset_test.txt">the ground truth</a>, <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/original_subset_test.txt">the answers generated by the original model</a>, and <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/fine-tuned_subset_test.txt">the answers generated by the fine-tuned model</a> to perform a manual evaluation on a subset of the test set. The original model, answers the given question correctly in <strong>36%</strong> of the test cases whereas the fine-tuned model correctly answers <strong>62%</strong> of the test cases. You can find the <a href="https://github.com/ppashakhanloo/legal-gemma/blob/main/manual_evaluation.csv">model's behavior for individual samples in a separate file</a>. What surprises me is that this significant improvement is a result of very little amount of effort.</p>

<h2>Conclusion and Considerations</h2>
As we saw in this simple exercise, fine-tuning an LLM can significantly boost its performance on domain-specific tasks. Keep in mind that:
<ol>
	<li>The results are only as good as the dataset used.</li>
	<li>Legal language changes; so we will need to update the model over time.</li>
	<li>We should be mindful of the potential biases and fairness issues in the model's output.</li>
</ol>
</details>


<details>
<summary><large>Quick Deep Learning Notes <small style="font-weight: lighter;"> January, 2022</small></large></summary>
<br>
<p style="font-weight: 200">
When I started exploring deep learning, I had to look up different sources for each topic; there are so many of them. Now that I have come to a conclusion about sources that work for me, I share some of these (hand-written) notes in case somebody is looking for fast-paced yet A-to-Z tutorials on Deep Learning and PyTorch.
Although typing all the notes is a thing now, I still prefer hand-written notes. So, please excuse my scribbles and doodles!
</p>


<ul>


<li> <strong>Basic Concepts</strong><br>
<ul>
<li><a href="./_posts/files/post_7/1_intro/1_BasicMLKeywords.pdf">Defining Some Terms</a></li>
<li><a href="./_posts/files/post_7/1_intro/2_NumpyIntro.pdf">Numpy Introduction</a></li>
<li><a href="./_posts/files/post_7/1_intro/3_Perceptron.pdf">Perceptron</a></li>
<li><a href="./_posts/files/post_7/1_intro/4_PerceptronAlgorithm.pdf">Perceptron Algorithm</a></li>
<li><a href="./_posts/files/post_7/1_intro/5_Errors.pdf">Errors</a></li>
<li><a href="./_posts/files/post_7/1_intro/6_GradientDescent.pdf">Gradient Descent</a></li>
<li><a href="./_posts/files/post_7/1_intro/7_NNArchitecture.pdf">Neural Network Architecture</a></li>
<li><a href="./_posts/files/post_7/1_intro/8_FeedForward.pdf">FeedForward</a></li>
<li><a href="./_posts/files/post_7/1_intro/9_BackPropagation.pdf">Back Propagation</a></li>
<li><a href="./_posts/files/post_7/1_intro/10_GradientDescentAlgorithm.pdf">Gradient Descent Algorithm</a></li>
<li><a href="./_posts/files/post_7/1_intro/11_MLP.pdf">Multi-layer Perceptron</a></li>
<li><a href="./_posts/files/post_7/1_intro/12_MulticlassClassification.pdf">Multiclass Classification</a></li>
<li><a href="./_posts/files/post_7/1_intro/13_MaximumLikelihood.pdf">Maximum Likelihood</a></li>
<li><a href="./_posts/files/post_7/1_intro/14_RandomRestart.pdf">Random Restart</a></li>
<li><a href="./_posts/files/post_7/1_intro/15_Dropout.pdf">Dropout</a></li>
<li><a href="./_posts/files/post_7/1_intro/16_Regularization.pdf">Regularization</a></li>
<li><a href="./_posts/files/post_7/1_intro/17_SentimentAnalysis.pdf">Sentiment Analysis</a></li>
</ul>
</li>


<li><strong>PyTorch Basics</strong><br>
<ul>
<li><a href="./_posts/files/post_7/2_pytorch/1_Tensors.pdf">Tensors</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/2_NeuralNetsInPyTorch.pdf">Neural Networks</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/3_nnModulePyTorch.pdf">Nueral Networks Module (<code>nn</code>)</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/4_TrainingNeuralNetsInPyTorch.pdf">Training Neural Networks in PyTorch</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/5_AvoidOverFitting.pdf">Avoiding Overfitting</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/6_SaveAndLoadModels.pdf">Save and Load Models</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/7_PyTorchImageData.pdf">Handling Image Data</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/8_TransferLearning.pdf">Transfer Learning</a></li>
<li><a href="./_posts/files/post_7/2_pytorch/9_FullNueralNetPyTorchExample.pdf">Full Nueral Network Example</a></li>
</ul>
</li>


<li><strong>Convolutional Neural Networks (CNN)</strong><br>
<ul>
<li><a href="./_posts/files/post_7/3_cnn/1_ImagesAsNumbers.pdf">Interpreting Images</a></li>
<li><a href="./_posts/files/post_7/3_cnn/2_KeyTerms.pdf">Some Terms</a></li>
<li><a href="./_posts/files/post_7/3_cnn/3_ConvLayers.pdf">Convolutional Layers</a></li>
<li><a href="./_posts/files/post_7/3_cnn/4_PyTorchInMNIST.pdf">MNIST in PyTorch</a></li>
<li><a href="./_posts/files/post_7/3_cnn/5_CNNComponents.pdf">CNN Components</a></li>
<li><a href="./_posts/files/post_7/3_cnn/6_CNNvsMLP.pdf">CNN versus MLP</a></li>
<li><a href="./_posts/files/post_7/3_cnn/7_ConvLayersPyTorch.pdf">Convolutional Layers in PyTorch</a></li>
<li><a href="./_posts/files/post_7/3_cnn/8_HighLevelConvLayer.pdf">Bird's Eye View of Convolutional Layers</a></li>
<li><a href="./_posts/files/post_7/3_cnn/9_HighLevelCNN.pdf">Bird's Eye View of CNN</a></li>
<li><a href="./_posts/files/post_7/3_cnn/10_CIFARinPyTorch.pdf">CIFAR in PyTorch</a></li>
<li><a href="./_posts/files/post_7/3_cnn/11_InvariantRepr.pdf">Invariant Representation</a></li>
<li><a href="./_posts/files/post_7/3_cnn/12_ResNetArch.pdf">ResNet Architecture</a></li>
<li><a href="./_posts/files/post_7/3_cnn/13_TransferLearningCheatsheet.pdf">Transfer Learning Cheatsheet</a></li>
<li><a href="./_posts/files/post_7/3_cnn/14_TransferLearningInPyTorch.pdf">Transfer Learning in PyTorch</a></li>
<li><a href="./_posts/files/post_7/3_cnn/15_InitialWeights.pdf">Initial Weights</a></li>
<li><a href="./_posts/files/post_7/3_cnn/16_AutoEncoders.pdf">Auto-encoders</a></li>
<li><a href="./_posts/files/post_7/3_cnn/17_StyleTransferInPyTorch.pdf">Style Transfer in PyTorch</a></li>
</ul>



<li><strong>Sequence-to-sequence Models</strong><br>
<ul>
<li><a href="./_posts/files/post_7/4_seq2seq/1_SomeTerms.pdf">Some Terms</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/2_RNN.pdf">Recurrent Neural Networks (RNN)</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/3_TrainingRNN.pdf">Training RNNs</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/4_RNNinPyTorch.pdf">RNN in PyTorch</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/5_LSTM.pdf">Long Short-term Memory Cell (LSTM)</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/6_AdvancedCells.pdf">More Advanced Cells</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/7_LSTMinPyTorch.pdf">LSTM in PyTorch</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/8_TuningHyperParams.pdf">Tuning Hyperparameters</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/9_SentimentAnalysisLSTMinPyTorch.pdf">LSTM Sentiment Analysis in PyTorch</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/10_Embedding.pdf">Embedding</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/11_Word2VecInPyTorch.pdf">Word2vec in PyTorch</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/12_Attention.pdf">Attention Mechanism</a></li>
<li><a href="./_posts/files/post_7/4_seq2seq/13_Transformer.pdf">Transformer</a></li>
</ul>

<li><strong>Graph Neural Networks</strong><br>
<ul>
<li><a href="./_posts/files/post_7/5_gnn/0_QuickGNNOverview.pdf">GNN Overview</a></li>
<li><a href="./_posts/files/post_7/5_gnn/1_ToyPyGExample.pdf">Toy PyTorch Geometric Example</a></li>
<li><a href="./_posts/files/post_7/5_gnn/2_GANinPyTorch.pdf">Graph Attention Networks in PyG</a></li>
</ul>

<li><strong>Generative Adversarial Networks</strong><br>
<ul>
<li><a href="./_posts/files/post_7/6_gan/0_GenAN.pdf">GAN Overview</a></li>
<li><a href="./_posts/files/post_7/6_gan/1_GANPyTorch.pdf">GAN in PyTorch</a></li>
<li><a href="./_posts/files/post_7/6_gan/2_DCGAN.pdf">Deep Convolutional GAN</a></li>
<li><a href="./_posts/files/post_7/6_gan/3_BatchNormalization.pdf">Batch Normalization</a></li>
<li><a href="./_posts/files/post_7/6_gan/4_DCGANPyTorch.pdf">DCGAN in PyTorch</a></li>
</ul>



<li><strong>Deep Graph Library (DGL)</strong><br>
<ul>
<li><a href="./_posts/files/post_7/7_dgl/0_NodeClassificationTorch.pdf">Node Classification Implementation</a></li>
<li><a href="./_posts/files/post_7/7_dgl/1_Graphs.pdf">Working with Graph Data</a></li>
<li><a href="./_posts/files/post_7/7_dgl/2_LinkPredictionTorch.pdf">Link Prediction Implementation</a></li>

<li><a href="./_posts/files/post_7/7_dgl/3_GraphClassificationTorch.pdf">Graph Classification Implementation (soon)</a></li>
<li><a href="./_posts/files/post_7/7_dgl/4_BuildGNNTorch.pdf">Implement a GNN (soon)</a></li>

</ul>



</ul>

</ul>








<h3>References:</h3>

<ul>
<li><a href="https://www.udacity.com/course/deep-learning-nanodegree--nd101">Udacity Deep Learning Nano-degree<a></li>
<li><a href="https://github.com/udacity/deep-learning-v2-pytorch">https://github.com/udacity/deep-learning-v2-pytorch<a></li>
<li><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow by Aurelien Geron<a></li>
<li><a href="https://www.deeplearningbook.org/">Deep Learning by Ian Goodfellow, Yoshua Benigo, and Aaron Courville</a></li>
<li><a href="https://www.youtube.com/watch?v=ABCGCf8cJOE">https://www.youtube.com/watch?v=ABCGCf8cJOE</a></li>
<li><a href="https://www.youtube.com/watch?v=-UjytpbqX4A">https://www.youtube.com/watch?v=-UjytpbqX4A</a></li>
<li><a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html">Colab Notebooks and Video Tutorials</a></li>
<li><a href="https://docs.dgl.ai/tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>

</ul>

<hr>
</details>


<details>
<summary><large>Neurips 2021 ML4Code-related Papers <small style="font-weight: lighter;"> January, 2022</small></large></summary>
<br>

<p style="font-weight: 200;">
Many interesting papers appear every year at Neurips. Only a small subset are considered <i>ml4code</i> or of interest to <i>ml4code</i> community. So, to make life easier, I made a list.
</p>


<ul>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_ed23fbf18c2cd35f8c7f8de44f85c08d.html">Unsupervised Translation of Programming Languages</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_5cd5058bca53951ffa7801bcdf421651.html">Multi-label Contrastive Predictive Coding</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_36ac8e558ac7690b6f44e2cb5ef93322.html">Feature Importance Ranking for Deep Learning</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_61d77652c97ef636343742fc3dcf3ba9.html">A Closer Look at Accuracy vs. Robustness</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_412604be30f701b1b1e3124c252065e6.html">Graph Meta Learning via Local Subgraphs</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_fe131d7f5a6b38b23cc967316c13dae2.html">PLANS: Neuro-Symbolic Program Learning from Videos</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/workshop_16122.html">Workshop: KR2ML - Knowledge Representation and Reasoning Meets Machine Learning</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_f6a8dd1c954c8506aadc764cc32b895e.html">Fast Transformers with Clustered Attention</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_22f791da07b0d8a2504c2537c560001c.html">Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_a512294422de868f8474d22344636f16.html">Prediction with Corrupted Expert Advice</a></li>
<li><a href="https://neurips.cc/virtual/2020/protected/poster_0004d0b59e19461ff126e3a08a814c33.html">A graph similarity for deep learning</a></li>
</ul>

<hr>
</details>

<details>
<summary><large>Deep Learning for Code: a Collection of Papers <small style="font-weight: lighter;"> December, 2021</small></large></summary>
<br>
<p style="font-weight: 200">
In this post, I try to list every ML/DL paper that targets code understanding, code representation, and bug finding. Despite my efforts to compile an exhaustive list, there are definitely ones that I have missed. Please email me if you find a paper that is missing. I will keep this post up-to-date as I continue my studies.
</p>

<p style="font-weight: 200">
Every top-level category consists of the papers that were published in a specific year. Under some of the paper descriptions, I have added a few keywords about the ideas and tasks.
</p>

<h2>2021</h2>

<ol>
<li> <strong style="font-weight:500">Language-agnostic representation learning of source code from structure and context</strong><br>
Zugner, Daniel and Kirschstein, Tobias and Catasta, Michele and Leskovec, Jure and Gunnemann, Stephan<br>
arXiv preprint arXiv:2103.11318<br>
ICLR<br>
<pre>{model: , tasks: , representation: , highlights: }</pre></li>

<li> <strong style="font-weight:500">Evaluating large language models trained on code</strong><br>
Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yura and Joseph, Nicholas and Brockman, Greg and others<br>
arXiv preprint arXiv:2107.03374</li>

<li> <strong style="font-weight:500">More with less: Exploring how to use deep learning effectively through semi-supervised learning for automatic bug detection in student code.</strong><br>
Shi, Yang and Mao, Ye and Barnes, Tiffany and Chi, Min and Price, Thomas W<br>
In Proceedings of the 14th International Conference on Educational Data Mining (EDM)</li>

<li> <strong style="font-weight:500">Fast and memory-efficient neural code completion</strong><br>
Svyatkovskiy, Alexey and Lee, Sebastian and Hadjitofi, Anna and Riechert, Maik and Franco, Juliana Vicente and Allamanis, Miltiadis<br>
2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)<br>
<pre>{model: , tasks: code completion, representation: embeds subtokens, highlights: }</pre></li>

<li> <strong style="font-weight:500">CCMC: Code Completion with a Memory Mechanism and a Copy Mechanism</strong><br>
Yang, Hao and Kuang, Li<br>
Evaluation and Assessment in Software Engineering<br>
<pre>{model: transformer-XL, tasks: , representation: sequence of AST nodes in in-order DFS fashion, highlights: long-range dependencies but consumes a lot of memory and compute resources}</pre></li>

<li> <strong style="font-weight:500">Studying the usage of text-to-text transfer transformer to support code-related tasks</strong><br>
Mastropaolo, Antonio and Scalabrino, Simone and Cooper, Nathan and Palacio, David Nader and Poshyvanyk, Denys and Oliveto, Rocco and Bavota, Gabriele<br>
2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)<br></li>

<li> <strong style="font-weight:500">InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees</strong><br>
Bui, Nghi DQ and Yu, Yijun and Jiang, Lingxiao<br>
2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)<br>
<pre>{model: Tree-based CNN, tasks: , representation: subtrees of an AST--traverses AST and for specific nodes such as stmts extracts subtree rooted at the visited node then makes a vocab of subtrees-->so the model's job would be to predict subtrees given an AST, highlights: unsupervised so helps with scarcity of labeled data, pretraining}</pre></li>

<li> <strong style="font-weight:500">PSIMiner: A Tool for Mining Rich Abstract Syntax Trees from Code</strong><br>
Spirin, Egor and Bogomolov, Egor and Kovalenko, Vladimir and Bryksin, Timofey<br>
arXiv preprint arXiv:2103.12778<br></li>

<li> <strong style="font-weight:500">A Survey on Software Defect Prediction Using Deep Learning</strong><br>
Akimova, Elena N and Bersenev, Alexander Yu and Deikov, Artem A and Kobylkin, Konstantin S and Konygin, Anton V and Mezentsev, Ilya P and Misilov, Vladimir E<br>
Multidisciplinary Digital Publishing Institute</li>

<li> <strong style="font-weight:500">A large-scale benchmark for few-shot program induction and synthesis</strong><br>
Alet, Ferran and Lopez-Contreras, Javier and Koppel, James and Nye, Maxwell and Solar-Lezama, Armando and Lozano-Perez, Tomas and Kaelbling, Leslie and Tenenbaum, Joshua<br>
International Conference on Machine Learning</li>

<li> <strong style="font-weight:500">On the Effectiveness of Deep Vulnerability Detectors to Simple Stupid Bug Detection</strong><br>
Hua, Jiayi and Wang, Haoyu<br>
2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR)</li>

<li> <strong style="font-weight:500">BERT2Code: Can Pretrained Language Models be Leveraged for Code Search?</strong><br>
Ishtiaq, Abdullah Al and Hasan, Masum and Haque, Md and Anjum, Mahim and Mehrab, Kazi Sajeed and Muttaqueen, Tanveer and Hasan, Tahmid and Iqbal, Anindya and Shahriyar, Rifat<br>
arXiv preprint arXiv:2104.08017<br>
<pre>{model: code2vec/codebert to embed source code and a simple NN with 2 hidden layers to embed NL query, tasks: , representation: , highlights: learns a mapping between NL embeddings and code embeddings}</pre></li>

<li> <strong style="font-weight:500">On the generalizability of Neural Program Models with respect to semantic-preserving program transformations</strong><br>
Rabin, Md Rafiqul Islam and Bui, Nghi DQ and Wang, Ke and Yu, Yijun and Jiang, Lingxiao and Alipour, Mohammad Amin<br>
Information and Software Technology<br>
<pre>{model: , tasks: , representation: , highlights: robustness study}</pre></li>

<li> <strong style="font-weight:500">Do Transformers Really Perform Bad for Graph Representation?</strong><br>
Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan<br>
arXiv preprint arXiv:2106.05234<br>
<pre>{model: transformer for graphs, tasks: , representation: , highlights: they call it Graphormer}</pre></li>

<li> <strong style="font-weight:500">TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer</strong><br>
Berabi, Berkay and He, Jingxuan and Raychev, Veselin and Vechev, Martin<br>
International Conference on Machine Learning (PMLR)<br>
<pre>{model: transformer, tasks: fixing code errors, representation: sequence of tokens, features: }</pre></li>

<li> <strong style="font-weight:500">Generating Adversarial Computer Programs using Optimized Obfuscations</strong><br>
Srikant, Shashank and Liu, Sijia and Mitrovska, Tamara and Chang, Shiyu and Fan, Quanfu and Zhang, Gaoyuan and O'Reilly, Una-May<br>
arXiv preprint arXiv:2103.11882<br>
<pre>{model: , tasks: , representation: , highlights: focuses on adversarial robustness}</pre></li>

<li> <strong style="font-weight:500">Self-Supervised Bug Detection and Repair</strong><br>
Allamanis, Miltiadis and Jackson-Flux, Henry and Brockschmidt, Marc<br>
arXiv preprint arXiv:2105.12787<br>
<pre>{model: GNN, tasks: , representation: AST augmented with control and dataflow edges, features: defines the graph as entities and relations, works better than CuBERT and GREAT on real bugs, self-supervised.}</pre></li>

<li> <strong style="font-weight:500">How could Neural Networks understand Programs?</strong><br>
Peng, Dinglan and Zheng, Shuxin and Li, Yatao and Ke, Guolin and He, Di and Liu, Tie-Yan<br>
arXiv preprint arXiv:2105.04297<br>
<pre>{model: transformer, tasks: , representation: control flow of LLVM IR, highlights: }</pre></li>

<li> <strong style="font-weight:500">Code prediction by feeding trees to transformers</strong><br>
Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish<br>
2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)<br>
<pre>{model: transformer, tasks: , representation: 1) token sequence or 2) AST node sequence in pre-order fashion or 3) root-to-leaf paths, highlights: makes the transformer aware of the syntactic structure of code i.e. improves self attention block similar to GREAT}</pre></li>

<li> <strong style="font-weight:500">CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model</strong><br>
Wang, Xin and Wang, Yasheng and Zhou, Pingyi and Xiao, Meng and Wang, Yadao and Li, Li and Liu, Xiao and Wu, Hao and Liu, Jin and Jiang, Xi<br>
arXiv preprint arXiv:2108.04556<br>
<pre>{model: , tasks: , representation: AST as sequence, highlights: pretraining, noise invariant code representation using contrastive learning by introducing noise into input sequence at training time, focus on robustness}</pre></li>

<li> <strong style="font-weight:500">CoTexT: Multi-task Learning with Code-Text Transformer</strong><br>
Phan, Long and Tran, Hieu and Le, Daniel and Nguyen, Hieu and Anibal, James and Peltekian, Alec and Ye, Yanfang<br>
arXiv preprint arXiv:2105.08645<br>
<pre>{model: transformer, tasks: , representation: sequence of tokens, highlights: focuses on NL-PL tasks, pretraining}</pre></li>

<li> <strong style="font-weight:500">A Mocktail of Source Code Representations</strong><br>
Vagavolu, Dheeraj and Swarna, Karthik Chandra and Chimalakonda, Sridhar<br>
arXiv preprint arXiv:2106.10918<br>
<pre>{model: , tasks: , representation: AST+CFG+PDG, highlights: an extension of code2vec}</pre></li>

<li> <strong style="font-weight:500">Program Synthesis with Large Language Models</strong><br>
Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles<br>
arXiv preprint arXiv:2108.07732</li>

<li> <strong style="font-weight:500">Automatic Code Generation using Pre-Trained Language Models</strong><br>
Ottens, Lizi and Perez, Luis and Viswanathan, Sudharshan<br>
arXiv preprint arXiv:2102.10535</li>

<li> <strong style="font-weight:500">SySeVR: A framework for using deep learning to detect software vulnerabilities</strong><br>
Li, Zhen and Zou, Deqing and Xu, Shouhuai and Jin, Hai and Zhu, Yawei and Chen, Zhaoxuan<br>
IEEE Transactions on Dependable and Secure Computing</li>
</ol>


<h2>2020</h2>
<ol>
<li> <strong style="font-weight:500">Structural language models of code</strong><br>
Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran<br>
International Conference on Machine Learning<br>
<pre>{model: , tasks: code generation, representation: paths from the root and leaves in AST, features: copy mechanism}</pre></li>

<li> <strong style="font-weight:500">DL-Droid: Deep learning based android malware detection using real devices</strong><br>
Alzaylaee, Mohammed K and Yerima, Suleiman Y and Sezer, Sakir<br>
Computers & Security, Elsevier<br>
<pre>{model: , tasks: malware detection, representation: , features: hand-engineered and heuristic-based}</pre></li>

<li> <strong style="font-weight:500">DRAST--A Deep Learning and AST Based Approach for Bug Localization</strong><br>
Sangle, Shubham and Muvva, Sandeep and Chimalakonda, Sridhar and Ponnalagu, Karthikeyan and Venkoparao, Vijendran Gopalan<br>
arXiv preprint arXiv:2011.03449<br>
<pre>{model: , tasks: , representation: AST, highlights: }</pre></li>

<li> <strong style="font-weight:500">Backdoors in neural models of source code</strong><br>
Ramakrishnan, Goutham and Albarghouthi, Aws<br>
arXiv preprint arXiv:2006.06841<br>
<pre>{model: , tasks: , representation: , highlights: adversarial robustness}</pre></li>

<li> <strong style="font-weight:500">Adversarial examples for models of code</strong><br>
Yefet, Noam and Alon, Uri and Yahav, Eran<br>
Proceedings of the ACM on Programming Languages (OOPSLA)<br>
<pre>{model: , tasks: , representation: , highlights: adversarial robustness}</pre></li>

<li> <strong style="font-weight:500">Semantic robustness of models of source code</strong><br>
Ramakrishnan, Goutham and Henkel, Jordan and Wang, Zi and Albarghouthi, Aws and Jha, Somesh and Reps, Thomas<br>
arXiv preprint arXiv:2002.03043<br>
<pre>{model: , tasks: , representation: , highlights: focuses on semantic robustness and training with semantic-preserving code transformations}</pre></li>

<li> <strong style="font-weight:500">Software vulnerability detection using deep neural networks: A survey</strong><br>
Lin, Guanjun and Wen, Sheng and Han, Qing-Long and Zhang, Jun and Xiang, Yang<br>
Proceedings of the IEEE</li>

<li> <strong style="font-weight:500">Approaches for Representing Software as Graphs for Machine Learning Applications</strong><br>
Romanov, Vitaly and Ivanov, Vladimir and Succi, Giancarlo<br>
2020 International Computer Symposium (ICS)</li>


<li> <strong style="font-weight:500">TranS^3: A transformer-based framework for unifying code summarization and code search</strong><br>
Wang, Wenhua and Zhang, Yuqun and Zeng, Zhengran and Xu, Guandong<br>
arXiv preprint arXiv:2003.03238<br>
<pre>{model: transformer, tasks: code search and summarization, representation: AST, highlights: unifying framework for both code searching and summarization}</pre></li>

<li> <strong style="font-weight:500">Multi-task learning based pre-trained language model for code completion</strong><br>
Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi<br>
Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering</strong><br>
<pre>{model: transformer, tasks: code completion, representation: , highlights: pretraining}</pre></li>

<li> <strong style="font-weight:500">Language Modelling for Source Code with Transformer-XL</strong><br>
Dowdell, Thomas and Zhang, Hongyu<br>
arXiv preprint arXiv:2007.15813<br>
<pre>{model: transformer-XL, tasks: , representation: , highlights: language modeling for source code, increase context size}</pre></li>

<li> <strong style="font-weight:500">Big code != big vocabulary: Open-vocabulary models for source code</strong><br>
Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea<br>
2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)</li>

<li> <strong style="font-weight:500">Scelmo: Source code embeddings from language models<br>
Karampatsis, Rafael-Michael and Sutton, Charles<br>
arXiv preprint arXiv:2004.13214</strong>

<li> <strong style="font-weight:500">DeepVS: an efficient and generic approach for source code modelling usage</strong><br>
Hussain, Yasir and Huang, Zhiqiu and Zhou, Yu and Wang, Senzhang<br>
Electronics Letters, Wiley Online Library</li>

<li> <strong style="font-weight:500">Dlfix: Context-based code transformation learning for automated program repair</strong><br>
Li, Yi and Wang, Shaohua and Nguyen, Tien N<br>
Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering<br>
<pre>{model: tree-based LSTM, tasks: , representation: AST, highlights: learning transformations to fix code instead of seq2seq}</pre></li>

<li> <strong style="font-weight:500">Compiler-based graph representations for deep learning models of code</strong><br>
Brauckmann, Alexander and Goens, Andr{<br>'es and Ertel, Sebastian and Castrillon, Jeronimo<br>
Proceedings of the 29th International Conference on Compiler Construction<br>
<pre>{model: GNN, tasks: , representation: AST and control flow edges, highlights: }</pre></li>

<li> <strong style="font-weight:500">Deep learning for source code modeling and generation: Models, applications, and challenges</strong><br>
Le, Triet HM and Chen, Hao and Babar, Muhammad Ali<br>
ACM Computing Surveys (CSUR)</li>

<li> <strong style="font-weight:500">Duplicate bug report detection and classification system based on deep learning technique</strong><br>
Kukkar, Ashima and Mohana, Rajni and Kumar, Yugal and Nayyar, Anand and Bilal, Muhammad and Kwak, Kyung-Sup<br>
IEEE Access</li>

<li> <strong style="font-weight:500">A self-attentional neural architecture for code completion with multi-task learning</strong><br>
Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Fu, Zhiyi and Jin, Zhi<br>
Proceedings of the 28th International Conference on Program Comprehension<br>
<pre>{model: , tasks: code completion, representation: AST nodes as an ordered sequences to root, highlights: }</pre></li>

<li> <strong style="font-weight:500">A transformer-based approach for source code summarization</strong><br>
Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei<br>
arXiv preprint arXiv:2005.00653<br>
<pre>{model: transformer, tasks: code summarization, representation: pairwise relationships between tokens based on AST, features: long-range}</pre></li>

<li> <strong style="font-weight:500">Software defect prediction via transformer</strong><br>
Zhang, Qihang and Wu, Bin<br>
2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)<br>
<pre>{model: transformer, tasks: , representation: AST, highlights: }</pre></li>

<li> <strong style="font-weight:500">Adversarial robustness for code</strong><br>
Bielik, Pavol and Vechev, Martin<br>
International Conference on Machine Learning (PMLR)<br>
<pre>{model: , tasks: , representation: , highlights: focus on adversarial robustness of code}</pre></li>

<li> <strong style="font-weight:500">Modular tree network for source code representation learning</strong><br>
Wang, Wenhan and Li, Ge and Shen, Sijie and Xia, Xin and Jin, Zhi<br>
ACM Transactions on Software Engineering and Methodology (TOSEM)<br>
<pre>{model: tree-LSTM, tasks: , representation: AST, highlights: it is a modular tree network extracted from AST}</pre></li>

<li> <strong style="font-weight:500">IntelliCode Compose: Code generation using transformer</strong><br>
Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel<br>
Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering<br>
<pre>{model: transformer, tasks: code generation, representation: , highlights: they call the model GPT-C}</pre></li>

<li> <strong style="font-weight:500">Automated vulnerability detection in source code using minimum intermediate representation learning</strong><br>
Li, Xin and Wang, Lu and Xin, Yang and Yang, Yixian and Chen, Yuling<br>
Applied Sciences, Multidisciplinary Digital Publishing Institute</li>

<li> <strong style="font-weight:500">Hoppity: Learning graph transformations to detect and fix bugs in programs</strong><br>
Dinella, Elizabeth and Dai, Hanjun and Li, Ziyang and Naik, Mayur and Song, Le and Wang, Ke<br>
International Conference on Learning Representations (ICLR)<br>
<pre>{model: GNN, tasks: fixing bugs, representation: AST with subtoken cache, highlights: }</pre></li>

<li> <strong style="font-weight:500">CodeBERT: A pre-trained model for programming and natural languages</strong><br>
Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others<br>
arXiv preprint arXiv:2002.08155<br>
<pre>{model: transformer, tasks: code search, representation: , highlights: bimodal pretrained model for PL/NL}</pre></li>

<li> <strong style="font-weight:500">GraphcodeBERT: Pre-training code representations with data flow</strong><br>
Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others<br>
arXiv preprint arXiv:2009.08366<br>
<pre>{model: transformer, tasks: code refinement, representation: , highlights: pretraining, similar to codebert but uses dataflow info at pretraining}</pre></li>
</ol>

<h2>2019</h2>
<ol>
<li> <strong style="font-weight:500">Synthetic datasets for neural program synthesis</strong><br>
Shin, Richard and Kant, Neel and Gupta, Kavi and Bender, Christopher and Trabucco, Brandon and Singh, Rishabh and Song, Dawn<br>
arXiv preprint arXiv:1912.12345<br>
2019</li>

<li> <strong style="font-weight:500">PathMiner: a library for mining of path-based representations of code</strong><br>
Kovalenko, Vladimir and Bogomolov, Egor and Bryksin, Timofey and Bacchelli, Alberto<br>
2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)<br>
2019</li>

<li> <strong style="font-weight:500">A hybrid deep learning image-based analysis for effective malware detection</strong><br>
Venkatraman, Sitalakshmi and Alazab, Mamoun and Vinayakumar, R<br>
Journal of Information Security and Applications, Elsevier<br>
2019</li>

<li> <strong style="font-weight:500">Pre-trained language model representations for language generation</strong><br>
Edunov, Sergey and Baevski, Alexei and Auli, Michael<br>
arXiv preprint arXiv:1903.09722<br>
2019</li>

<li> <strong style="font-weight:500">Multi-modal attention network learning for semantic source code retrieval</strong><br>
Wan, Yao and Shu, Jingdong and Sui, Yulei and Xu, Guandong and Zhao, Zhou and Wu, Jian and Yu, Philip S<br>
arXiv preprint arXiv:1909.13516<br>
2019</li>

<li> <strong style="font-weight:500">A zero-positive learning approach for diagnosing software performance regressions</strong><br>
Alam, Mejbah and Gottschlich, Justin and Tatbul, Nesime and Turek, Javier S and Mattson, Tim and Muzahid, Abdullah<br>
Advances in Neural Information Processing Systems<br>
2019</li>

<li> <strong style="font-weight:500">code2vec: Learning distributed representations of code</strong><br>
Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran<br>
Proceedings of the ACM on Programming Languages<br>
<pre>{model: , tasks: , representation: pairwise paths between AST terminal nodes, highlights: }</pre></li>

<li> <strong style="font-weight:500">Deep-autocoder: Learning to complete code precisely with induced code tokens</strong><br>
Hu, Xing and Men, Rui and Li, Ge and Jin, Zhi<br>
2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)<br>
<pre>{model: LSTM, tasks: , representation: AST, highlights: learn language models over code corpus}</pre></li>

<li> <strong style="font-weight:500">Pythia: AI-assisted code completion system</strong><br>
Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel<br>
Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining<br>
<pre>{model: LSTM, tasks: code completion, representation: serialized AST, highlights: }</pre></li>

<li> <strong style="font-weight:500">Maybe deep neural networks are the best choice for modeling source code</strong><br>
Karampatsis, Rafael-Michael and Sutton, Charles<br>
arXiv preprint arXiv:1903.05734</li>

<li> <strong style="font-weight:500">Structural language models for any-code generation</strong><br>
Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran</li>

<li> <strong style="font-weight:500">A novel neural source code representation based on abstract syntax tree</strong><br>
Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong<br>
2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)<br>
<pre>{model: RNN, tasks: , representation: AST split into a sequence of small stmt-level subtrees, highlights: }</pre></li>

<li> <strong style="font-weight:500">IdBench: Evaluating Semantic Representations of Identifier Names in Source Code</strong><br>
Wainakh, Yaza and Rauf, Moiz and Pradel, Michael<br>
arXiv preprint arXiv:1910.05177</li>

<li> <strong style="font-weight:500">Neural program repair by jointly learning to localize and repair</strong><br>
Vasic, Marko and Kanade, Aditya and Maniatis, Petros and Bieber, David and Singh, Rishabh<br>
arXiv preprint arXiv:1904.01720</li>

<li> <strong style="font-weight:500">Open vocabulary learning on source code with a graph-structured cache</strong><br>
Cvitkovic, Milan and Singh, Badal and Anandkumar, Animashree<br>
International Conference on Machine Learning (PMLR)<br>
<pre>{model: GNN, tasks: , representation: augmented AST, highlights: add a graph-structural vocabulary cache to the graph--add edges from a subtoken vocab to terminal nodes}</pre></li>

<li> <strong style="font-weight:500">A literature study of embeddings on source code</strong><br>
Chen, Zimin and Monperrus, Martin<br>
arXiv preprint arXiv:1904.03061</li>

<li> <strong style="font-weight:500">Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks</strong><br>
Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang<br>
arXiv preprint arXiv:1909.03496<br>
<pre>{model: GNN, tasks: , representation: AST, highlights: }</pre></li>

<li> <strong style="font-weight:500">Global Relational Models of Source Code</strong><br>
Hellendoorn, Vincent J and Sutton, Charles and Singh, Rishabh and Maniatis, Petros and Bieber, David<br>
International conference on learning representations<br>
<pre>{model: transformer with attention bias, tasks: varmisuse, representation: sequence of tokens but incorporating semantically meaningful relations, highlights: longer-range dependencies compared to transformer but still limited by context-size}</pre></li>

<li> <strong style="font-weight:500">Learning a static bug finder from data</strong><br>
Wang, Yu and Gao, Fengjuan and Wang, Linzhang and Wang, Ke<br>
arXiv preprint arXiv:1907.05579<br>
<pre>{model: GNN, tasks: , representation: augmented AST, highlights: split the code graph into multiple disjoint ones, suitable for more complex bugs such as null pointer deref, less accurate when handling large programs (large = 1000 nodes)}</pre></li>

<li> <strong style="font-weight:500">Deep learning for bug-localization in student programs</strong><br>
Gupta, Rahul and Kanade, Aditya and Shevade, Shirish<br>
arXiv preprint arXiv:1905.12454<br>
<pre>{model: tree convolutional neural network, tasks: bug localization, representation: AST, highlights: }</pre></li>

<li> <strong style="font-weight:500">A comprehensive study on deep learning bug characteristics</strong><br>
Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh<br>
Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</li>

<h2>2018</h2>
<li> <strong style="font-weight:500">Deepbugs: A learning approach to name-based bug detection</strong><br>
Pradel, Michael and Sen, Koushik<br>
Proceedings of the ACM on Programming Languages (OOPSLA)<br>
<pre>{model: , tasks: , representation: embed only program identifiers in a list, highlights: }</pre></li>

<li> <strong style="font-weight:500">code2seq: Generating sequences from structured representations of code</strong><br>
Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran<br>
arXiv preprint arXiv:1808.01400<br>
<pre>{model: , tasks: code captioning, representation: all pairwise paths between terminal nodes in AST, features: }</pre></li>
</ol>


<h2>2017</h2>
<ol>
<li> <strong style="font-weight:500">Learning to Represent Student Knowledge on Programming Exercises Using Deep Learning</strong><br>
Wang, Lisa and Sy, Angela and Liu, Larry and Piech, Chris<br>
International Educational Data Mining Society</li>

<li> <strong style="font-weight:500">Pallas: Semantic-aware checking for finding deep bugs in fast path</strong><br>
Huang, Jian and Allen-Bond, Michael and Zhang, Xuechen<br>
Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</li>

<li> <strong style="font-weight:500">Learning to represent programs with graphs</strong><br>
Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud<br>
arXiv preprint arXiv:1711.00740<br>
<pre>{model: GGNN, tasks: varmisuse, representation: AST augmented with control and dataflow, features: }</pre></li>

<li> <strong style="font-weight:500">DeepFix: Fixing common C language errors by deep learning</strong><br>
Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish<br>
Thirty-First AAAI Conference on Artificial Intelligence<br>
<pre>{model: multi-layered seq2seq neural network with attention, tasks: bug finding and fixing, representation: token sequence, highlights: }</pre></li>

<li> <strong style="font-weight:500">Inductive representation learning on large graphs</strong><br>
Hamilton, William L and Ying, Rex and Leskovec, Jure<br>
Proceedings of the 31st International Conference on Neural Information Processing Systems</li>

<li> <strong style="font-weight:500">Code completion with neural attention and pointer networks</strong><br>
Li, Jian and Wang, Yue and Lyu, Michael R and King, Irwin<br>
arXiv preprint arXiv:1711.09573<br>
<pre>{model: , tasks: , representation: flattened AST, highlights: }</pre></li>

<li> <strong style="font-weight:500">Program synthesis from natural language using recurrent neural networks</strong><br>
Lin, Xi Victoria and Wang, Chenglong and Pang, Deric and Vu, Kevin and Ernst, Michael D<br>
University of Washington Department of Computer Science and Engineering, Seattle, WA, USA, Tech. Rep. UW-CSE-17-03-01<br>
<pre>{model: RNN encoder-decoder, tasks: program synthesis from NL, representation: , highlights: generates a program template from NL sentence}</pre></li>
</ol>

<h2>2016</h2>
<ol>
<li> <strong style="font-weight:500">Program synthesis using natural language</strong><br>
Desai, Aditya and Gulwani, Sumit and Hingorani, Vineet and Jain, Nidhi and Karkare, Amey and Marron, Mark and Roy, Subhajit<br>
Proceedings of the 38th International Conference on Software Engineering</li>

<li> <strong style="font-weight:500">Neuro-symbolic program synthesis</strong><br>
Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet<br>
arXiv preprint arXiv:1611.01855</li>

<li> <strong style="font-weight:500">Summarizing source code using a neural attention model</strong><br>
Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke<br>
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</li>
</ol>

<h2>2013</h2>

<ol>
<li> <strong style="font-weight:500">Mantis: Automatic performance prediction for smartphone applications</strong><br>
Kwon, Yongin and Lee, Sangmin and Yi, Hayoon and Kwon, Donghyun and Yang, Seungjun and Chun, Byung-Gon and Huang, Ling and Maniatis, Petros and Naik, Mayur and Paek, Yunheung<br>
USENIX Annual Technical Conference 13</li>
</ol>

<hr>
</details>

<details>
<summary><large>Public Datasets for Code Understanding Tasks <small style="font-weight: lighter;"> December, 2021</small></large></summary>
<br>
<p style="font-weight: 200">
Labeled data is typically scarce for code-understanding tasks, both for training and evaluation purposes. In this post, I share a number of datasets that ML/SE researchers have collected and (kindly) shared. I will keep this post up-to-date as I find new datsets. Please email me if some dataset is missing.
</p>

<h2>Python</h2>

<ol>
<li>QuickBugs<br>
<a href="https://jkoppel.github.io/QuixBugs/">https://jkoppel.github.io/QuixBugs/</a><br>
<a href="https://jkoppel.github.io/QuixBugs/quixbugs.pdf">https://jkoppel.github.io/QuixBugs/quixbugs.pdf</a>
</li>

<li>BugSwarm<br>
<a href="http://www.bugswarm.org/dataset/">http://www.bugswarm.org/dataset/</a><br>
<a href="https://web.cs.ucdavis.edu/~rubio/includes/icse19.pdf">https://web.cs.ucdavis.edu/~rubio/includes/icse19.pdf</a>
</li>

<li>
BugsInPy<br>
<a href="https://github.com/soarsmu/BugsInPy">https://github.com/soarsmu/BugsInPy</a><br>
<a href="https://dl.acm.org/doi/pdf/10.1145/3368089.3417943">https://dl.acm.org/doi/pdf/10.1145/3368089.3417943</a>
</li>

<li>
refactory<br>
<a href="https://github.com/githubhuyang/refactory">https://github.com/githubhuyang/refactory</a><br>
<a href="https://ieeexplore.ieee.org/abstract/document/8952522">https://ieeexplore.ieee.org/abstract/document/8952522</a>
</li>

<li>
Misc.<br>
<a href="https://www.kaggle.com/saitejaponugoti/deepbugs-for-python">https://www.kaggle.com/saitejaponugoti/deepbugs-for-python</a>
</li>
</ol>

<h2>Java</h2>

<ol>
<li>
XCorpus<br>
<a href="https://bitbucket.org/jensdietrich/xcorpus/src/master">https://bitbucket.org/jensdietrich/xcorpus/src/master</a><br>
<a href="http://www.jot.fm/issues/issue_2017_04/article1.pdf">http://www.jot.fm/issues/issue_2017_04/article1.pdf</a>
</li>
</ol>

<hr>
</details>

<details>
<summary><large>Perceptron, MLP, and Backpropagation <small style="font-weight: lighter;"> October 10, 2021</small></large></summary>
<br>

<h2>Perceptron</h2>
<p style="font-weight: 200">
Perceptron is a small unit in neural networks. It takes <i>n</i> inputs, computes a weighted sum of them, and applies a step function or one of its variants on the weighted sum. The goal of training is to find weights that minimize the prediction error.
</p>

<p style="font-weight: 200">
We are going to create a simple classifier using Perceptron that predicts the language (among C, C#, C++, D, Haskell, Java, JS, PHP, Python, and Rust) in which a program is written. For this simple example I just use the number of occurences of 22 tokens as features.
</p>

<p style="font-weight: 200">
You can download the test and train data from the following links: <a href="./_posts/files/post_3/proglang_test.csv">proglang_test</a> <a href="./_posts/files/post_3/proglang_train.csv">proglang_train</a>
</p>

<p style="font-weight: 200">
The following lines of code, train a multi-class classifier and evaluate it.
</p>

<pre>
import pandas
from sklearn.linear_model import Perceptron

data_test = pandas.read_csv('proglang_test.csv')
data_train = pandas.read_csv('proglang_train.csv')

# feature columns
X_test = data_test.iloc[:, 0:-2]
X_train = data_train.iloc[:, 0:-2]

# labels
y_test = data_test.iloc[:, -2] 
y_train = data_train.iloc[:, -2]

# train the classifier
classifier = Perceptron()
classifier.fit(X_train, y_train)

# evaluate the classifier
print('Mean accuracy:', classifier.score(X_test, y_test))
</pre>

<pre>
Mean accuracy: 0.95
</pre>


<h3>Multi-layer Perceptron</h3>
<p style="font-weight: 200">
A multi-layer perceptron has 1) one input layer, 2) a number of hidden layers, and 3) an output layer. The building blocks in this kind if network is the Perceptron. Each layer is fully connected to the next layer. The message passes in one direction (from lower layer to upper layers) so it is conviniently called <i>feed forward</i> network. If there are several hidden layers then it is called a <i>deep</i> network.
</p>

<h3>Backpropagation</h3>
<p style="font-weight: 200">
After the weights of the model is set to random values, one small batch of samples are run through the network in <i>forward</i> direction until it makes the final predictions. Then, the error is measured in reverse---or <i>backwards</i>---for each layer with the goal of finding how much each connection contributes to the error. Finally, the weights are re-adjusted based on the computed values.
As I mentioned earlier, in the original Perceptron, a step function was used at the final step. In modern MLPs, however, this step function is replaced by other <i>activation</i> functions such as <i>tanh</i> and <i>relu</i>.
</p>

<h2>Reference</h2>
<ul>
<li>Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O'Reilly Media, 2019.</li>
</ul>

<hr>
</details>

<details>
<summary><large>Hardening C/C++ Code with Clang Sanitizers <small style="font-weight: lighter;"> May 10, 2020</small></large></summary>
<br>
<p style="font-weight: 200">
Many common programming errors manifest as violations of general safety properties such as memory safety (e.g. null pointer dereferencing) and concurrency safety (e.g. data races). Certain languages such as Rust aim to <i>statically</i> check such properties but impose additional burden on developers. Managed languages such as Java and C# <i>dynamically</i> check such properties and throw exceptions when they are violated.  But most languages in widespread use, notably C and C++, offer very limited checking at compile-time and run-time. Unchecked safety violations in C/C++ programs are notorious for ill effects ranging from hard-to-debug runtime crashes to security exploits.
</p>

<p style="font-weight: 200">
Sanitizers are a wide-ranging solution to these problems. They bring C/C++ closer to safer languages like Java and Rust. Major C/C++ compilers such as GCC and Clang provide sanitizers. In this article, we will learn about Clang sanitizers.
</p>

<p style="font-weight: 200">
Clang sanitizers are safety checkers supported by Clang. They are compiler add-ons that insert checks by instrumenting the code in order to detect common programming errors at runtime. Four of these sanitizers are widely used in practice and have been successfully applied to large applications such as Google Chrome and Mozilla Firefox. These are: Address Sanitizer, Thread Sanitizer, Undefined Behavior Sanitizer, and Memory Sanitizer. The remainder of this article provides an overview of each of these sanitizers and demonstrates how you can apply them to harden C/C++ programs.
</p>

<p style="font-weight: 200">
An overview of the LLVM workflow is illustrated in the following block diagram. The Clang Sanitizer pass is located between the compiler front-end and the back-end, transforming an unsafe program to a safe one by instrumenting the program IR. This framework supports multiple frontends (e.g., C/C++ and Objective C/C++) as well as multiple backends (e.g., x86, x86_64, ARM, MIPS, etc.)
</p>

<h2>Major Clang Sanitizers</h2>

<p style="font-weight: 200">
<h3> 1. Address Sanitizer (ASan)</h3>
<a href="https://clang.llvm.org/docs/AddressSanitizer.html">Address Sanitizer</a> is a fast memory bug checker with a typical slowdown of 2x. It can detect the following kinds of bugs:
</p>

<p style="font-weight: 200">
<ul>
<li>Heap buffer overflow: A buffer overflow where the buffer is allocated in the heap portion of memory.</li>
<li>Stack buffer overflow: A condition when a program accesses a memory address on the program’s call stack outside of the intended data structure.</li>
<li>Use after free: An attempt to access memory after it has been freed.</li>
<li>Double/invalid free: An attempt to deallocate freed memory.</li>
<li>Memory leaks: Failure to release memory that is no longer needed.</li>
</ul>
</p>

<h3>2. Thread Sanitizer (TSan)</h3>

<p style="font-weight: 200">
<a href="https://clang.llvm.org/docs/ThreadSanitizer.html">Thread Sanitizer</a> is a checker that detects data races. A data race happens when two threads access the same resource concurrently and at least one of the accesses is a write. Data races are one of the most challenging bugs to detect in concurrent systems. Thread Sanitizer incurs an average slowdown of 2x-20x and an average memory overhead of 5x-10x.
</p>

<h3>3. Undefined Behavior Sanitizer (UBSan)</h3>
<p style="font-weight: 200">
<a href="https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html">Undefined Behavior Sanitizer</a> is a fast undefined behavior detector. Some examples of undefined behavior that it catches are as follows:
</p>

<ol>
<li>Misaligned pointer or reference</li>
<li>Out of bounds array indexing where the array bound can be statically determined</li>
<li>Implicit and explicit conversions between floating-points and integers</li>
<li>Division by zero</li>
<li>Performing pointer arithmetic which overflows</li>
<li>Signed and unsigned integer overflow</li>
<li>If control flow reaches an unreachable program point</li>
</ol>


<h3>4. Memory Sanitizer (MSan)</h3>
<p style="font-weight: 200">
<a href="https://clang.llvm.org/docs/MemorySanitizer.html">Memory Sanitizer</a> is a checker for uninitialized memory reads by tracking uninitialized bits in the memory. Uninitialized reads happen when the allocated memory is read before it is written. MSan tracks uninitialized bits. This sanitizer imposes an average slowdown of 3x, and an average memory overhead of 2x.
</p>


<h2>Quick Guide to Run Clang Sanitizers</h2>

<h3>1. Address Sanitizer (ASan)</h3>
<p style="font-weight: 200">
Create a new file <code>asan.cc</code>, and copy the following program into it:
</p>
<pre>
// asan.cc
1: int main(int argc, char **argv) {
2:   int *array = new int[100];
3:   delete [] array;
4:   return array[argc];
5: }
</pre>
<p style="font-weight: 200">
Next, compile and link the program with the <code>-fsanitize=address</code> flag:
</p>
<pre>
$ clang++ -g -fsanitize=address asan.cc
</pre>

<p style="font-weight: 200">
For better performance, add the -O1 optimization flag, and to get a nicer stack trace in the error message, add the <code>-fno-omit-frame-pointer</code> flag:
</p>
<pre>
$ clang++ -O1 -g -fsanitize=address -fno-omit-frame-pointer asan.cc
</pre>

<p style="font-weight: 200">
Next, simply run the compiled binary:
</p>

<pre>
$ ./a.out
</pre>

<p style="font-weight: 200">
If you follow the steps successfully, you will see the following error message printed to stderr, and the program will exit with a non-zero exit code.
</p>

<pre>
==37==ERROR: AddressSanitizer: heap-use-after-free on address 0x614000000044 at pc 0x0000004f8d5f bp 0x7ffe692d6990 sp 0x7ffe692d6988
READ of size 4 at 0x614000000044 thread T0
    #0 0x4f8d5e in main /sans/asan.cc:4:10
    #1 0x7f0500759b96 in __libc_start_main /build/glibc-OTsEL5/glibc-2.27/csu/../csu/libc-start.c:310
    #2 0x41af59 in _start (/sans/a.out+0x41af59)
...

</pre>
<p style="font-weight: 200">
In this example, the program is trying to access an array element after it has been deleted. So the sanitizer reports a heap-use-after-free error at line 4.
</p>

<h3>2. Thread Sanitizer (TSan)</h3>
<p style="font-weight: 200">
Create a new file <code>tsan.c</code>, and copy the following program into it:
</p>

<pre>
// tsan.c
 1: #include &lt;pthread.h&gt;
 2: int Global;
 3: void *Thread1(void *x) {
 4:   Global = 42;
 5:   return x;
 6: }
 7: int main() {
 8:   pthread_t t;
 9:   pthread_create(&t, NULL, Thread1, NULL);
10:   Global = 43;
11:   pthread_join(t, NULL);
12:   return Global;
13: }
</pre>

<p style="font-weight: 200">
Next, compile and link the program with the <code>-fsanitize=thread</code> flag:
</p>
<pre>
$ clang -fsanitize=thread -g -O1 tsan.c
</pre>

<p style="font-weight: 200">
Next, simply run the compiled binary. You might need to run it multiple times to encounter a data race. Later in the course, you will see how a test input generation tool such as AFL or LibFuzzer can automate this repetitive task.
</p>
<pre>
$ ./a.out
</pre>

<p style="font-weight: 200">
When the bug is encountered, the following error message will be printed to stderr:
</p>
<pre>
WARNING: ThreadSanitizer: data race (pid=145)
  Write of size 4 at 0x000001108278 by main thread:
    #0 main /sans/tsan.c:10:10 (a.out+0x4ac64e)

  Previous write of size 4 at 0x000001108278 by thread T1:
    #0 Thread1 /sans/tsan.c:4:10 (a.out+0x4ac607)

  Location is global 'Global' of size 4 at 0x000001108278 (a.out+0x000001108278)

  Thread T1 (tid=147, finished) created by main thread at:
    #0 pthread_create &lt;null&gt; (a.out+0x422fe5)
    #1 main /sans/tsan.c:9:3 (a.out+0x4ac644)

SUMMARY: ThreadSanitizer: data race /sans/tsan.c:10:10 in main
==================
ThreadSanitizer: reported 1 warnings
</pre>

<h3>3. Undefined Behavior Sanitizer (UBSan)</h3>
<p style="font-weight: 200">
Create a new file <code>ubsan.c</code>, and copy the following program into it:
</p>
<pre>
// ubsan.c
 1: #include &lt;inttypes.h&gt;
 2: #include &lt;stdint.h&gt;
 3: #include &lt;stdio.h&gt;
 4: #include &lt;string.h&gt;
 5: 
 6: int32_t convert(const uint8_t *restrict p) {
 7:   uint32_t x = (256*p[1] + 256*256*p[2] + 256*256*256*p[3]);
 8:   if (x > INT32_MAX) return (x - INT32_MAX) - 1;
 9:   else return (((int32_t)x + (int32_t)-INT32_MAX) - 1);
10: }
11: 
12: int main() {
13:   uint32_t value;
14:   uint8_t buf[sizeof(uint32_t)];
15:   while (scanf("%" SCNx32, &value) == 1) {
16:     memcpy(buf, &value, sizeof(buf));
17:     printf("%08" PRIx32 "\n", convert(buf));
18:   }
19:   return 0;
20: }
</pre>

<p style="font-weight: 200">
Next, compile and link the program with the <code>-fsanitize=undefined</code> flag:
</p>
<pre>
$ clang -g -fsanitize=undefined ubsan.c
</pre>

<p style="font-weight: 200">
Then, run the compiled binary with the following input:
</p>
<pre>
$ ./a.out
80000001
</pre>

<p style="font-weight: 200">
You should be able to witness the following runtime error:
</p>
<pre>
ubsan.c:7:54: runtime error: signed integer overflow: 16777216 * 128 cannot be represented in type 'int'
</pre>

<p style="font-weight: 200">
After reading <code>80000001</code>, UBSan detects an error and prints an error message. It points out the signed integer overflow in <code>256 * 256 * 256 * p[3]. p[3]</code> is an unsigned char which will convert to a signed integer between 0 to 255. Then, it is multiplied by <code>256 * 256 * 256</code>. In some cases, such as the case where the input is <code>80000001</code>, a signed integer overflow occurs which is considered an undefined behavior in C/C++.
</p>

<h3>4. Memory Sanitizer (MSan)</h3>
<p style="font-weight: 200">
Create a new file <code>msan.cc</code>, and copy the following program into it:
</p>

<pre>
// msan.cc
1: #include &lt;stdio.h&gt;
2: 
3: int main(int argc, char** argv) {
4:   int* a = new int[10];
5:   a[5] = 0;
6:   if (a[argc])
7:     printf("xx\n");
8:   return 0;
9: }
</pre>

<p style="font-weight: 200">
Next, compile and link the program with the <code>-fsanitize=memory</code> flag:
</p>
<pre>
$ clang++ -g -fsanitize=memory msan.cc
</pre>

<p style="font-weight: 200">
For better performance, add the <code>-O2</code> optimization flag, and to get a stack trace in the error message, add the <code>-fno-omit-frame-pointer</code> flag:
</p>
<pre>
$ clang++ -g -O2 -fno-omit-frame-pointer -fsanitize=memory msan.cc
</pre>

<p style="font-weight: 200">
Next, simply run the compiled binary:
</p>
<pre>
$ ./a.out
</pre>

<p style="font-weight: 200">
When the bug is encountered, the following error message will be printed to stderr:
</p>
<pre>
==52==WARNING: MemorySanitizer: use-of-uninitialized-value
    #0 0x495d7e in main /cis547vm/msan.cc:6:8
    #1 0x7f1d69194b96 in __libc_start_main /build/glibc-OTsEL5/glibc-2.27/csu/../csu/libc-start.c:310
    #2 0x41b7e9 in _start (/cis547vm/j+0x41b7e9)

SUMMARY: MemorySanitizer: use-of-uninitialized-value /cis547vm/msan.cc:6:8 in main
</pre>
<p style="font-weight: 200">
In this example, an uninitialized value is accessed in line 6, column 8.
</p>

<hr>
</details>

<details>
<summary><large>The Checker Framework <small style="font-weight: lighter;"> April 21, 2020</small></large></summary>
<br>
<p style="font-weight: 200;">
If you are interested in preventing bugs in your Java project, look no further. The Checker Framework improves Java’s type system through annotations to detect and prevent errors in Java programs. It includes compiler checkers that find bugs or verify the absence of them. The Checker Framework is widely used at Google, Amazon, and Uber.
</p>

<p style="font-weight: 200;">
The built-in type checker in Java, does provide some reasonable type checking and error prevention. However, the Checker Framework lets you define new type checkers and extend the type system and run them as a part of the javac compiler. The checker framework allows the developers to use the available type systems in the framework, or design their own. In this article, we will learn about the most popular checkers in this framework.
</p>

<h2>Nullness Checker</h2>
<p style="font-weight: 200;">
The most popular checker in the Checker Framework is the <a href="https://checkerframework.org/manual/#nullness-checker">Nullness Checker</a>. If this checker does not issue any warnings at compile time, it is guaranteed that the program will never throw a null pointer exception. The Nullness Checker is a standard part of the build system at Google.
</p>

<p style="font-weight: 200;">
The most important annotations supported by the Nullness Checker are the following:<br>
<ul>
<li><a href="https://checkerframework.org/api/org/checkerframework/checker/nullness/qual/Nullable.html"><code>@Nullable</code></a>: this annotation indicates a type that has <code>null</code> value in its possible set of values. For example, the possible set of values for a <code>Boolean</code> is <code>{true, false, null}</code>.</li>
<li><a href="https://checkerframework.org/api/org/checkerframework/checker/nullness/qual/NonNull.html"><code>@NonNull</code></a>: this annotation indicates a type that does not have <code>null</code> in its possible set of values. For example, a variable of type <code>@NonNull Boolean</code> only has true and false in its possible set of values.</li>
</ul>
</p>

<p style="font-weight: 200;">
This checker emits warnings in the following cases:<br>
<ul>
<li>When you dereference an expression that is not <code>@NonNull</code>.</li>
<li>When the value of a <code>@NonNull</code> expression evaluates to <code>null</code>.</li>
<li>When you assign <code>null</code> to a <code>@NonNull</code> variable.</li>
<li>When a field of type <code>@NonNull</code> is not initialized in a constructor.</li>
</ul>
</p>

<strong style="font-weight:500;">Example</strong>
<pre>
@Nullable Object object;
@NonNull Object nonNullObject;
…
object.toString(); // warning: possible null dereference
nonNullObject = object; // warning: nonNullObject might become null
if (nonNullObject == null) doSomething; // warning: redundant check
</pre>


<h2>GUI Effect Checker</h2>
<p style="font-weight: 200;">
When checking graphical user interfaces, the most prevalent bug is invalid thread access by a background process. Well-known GUI frameworks, such as Android and Swing, spawn a single main thread which is known as the UI event thread that is responsible for handling all the events and updates. Any other operation, such as expensive computations, is offloaded to background threads which are known as worker threads. The worker thread should send a request to the main thread to perform any access on its behalf. If any of these background threads directly access a UI element, the framework throws an exception and terminates the application.
</p>

<p style="font-weight: 200;">
This is a challenging issue to debug since it is difficult to remember which methods are permitted to be called on which threads. The <a href="https://checkerframework.org/manual/#guieffect-checker">GUI Effect Checker</a> solves this problem by allowing the programmer to annotate each method to indicate whether it accesses no UI elements, or whether it may access UI elements. The former method is said to have the safe effect and the latter is said to have the UI effect. A method with the safe effect is prohibited from calling a method with the UI effect.
</p>

<p style="font-weight: 200;">
The most important annotations supported by the GUI Effect Checker are the following:<br>
<ul>
<li><a href="https://checkerframework.org/api/org/checkerframework/checker/guieffect/qual/SafeEffect.html"><code>@SafeEffect</code></a>: this annotation indicates that a method is not allowed to access UI elements.</li>
<li><a href="https://checkerframework.org/api/org/checkerframework/checker/guieffect/qual/UIEffect.html"><code>@UIEffect</code></a>: this annotation indicates that a method may access UI elements. Note that it is always safe to call a <code>@SafeEffect</code> method whenever it is permitted to call a <code>@UIEffect</code> method.</li>
</ul>
</p>

<p style="font-weight: 200;">
This checker complains in the following cases:<br>
<ul>
<li>When a <code>@UIEffect</code> method is invoked by a <code>@SafeEffect</code> method.</li>
<li>When a supertype declares a @<code>SafeEffect</code> method, and its subtype is overridden as <code>@UIEffect</code>.</li>
<li>When a method implements or overrides a method in two supertypes and they have different GUI Effect annotations.</li>
</ul>
</p>

<strong style="font-weight:500;">Example</strong>
<pre>
@SafeEffect
public void foo(JTextField tf) {
  tf.setText(“first”); // error since setText is a @UIEffect method,
                       // and is called from a @SafeEffect method.
  Display.syncExec(new Runnable {
    @UIEffect
    public void run() {
      tf.setText(“second”);
    }
  });
}
</pre>


<h2>Tainting Checker</h2>
<p style="font-weight: 200;">
The <a href="https://checkerframework.org/manual/#tainting-checker">Tainting Checker</a> prevents specific kinds of errors that arise from untrusted or tainted values. These untrusted values come from possibly malicious sources such as user input or unvalidated data. Trust-related errors might cause the application to crash, corrupt data, or leak information. If the checker does not issue any warnings for a correctly-tainted program, no tainted values ever flow to a sensitive sink.
</p>

<p style="font-weight: 200;">
A program must sanitize any untrusted value before using it at any sensitive point in the program. In general, there are two ways to sanitize a value: 1. checking if it is valid, or 2. transforming the value to become valid. An example of the former is to check if the value contains no characters that can be interpreted as SQL commands. An example of the latter is to quote all the characters that can be interpreted as SQL commands.
</p>

<p style="font-weight: 200;">
The Tainting Checker is not aware of the internal semantics of the application, so it cannot warn you correctly if you mis-annotate. The mis-annotation happens in two ways: 1. a sensitive sink annotated as <code>@Tainted</code> data, or 2. external data annotated as <code>@Untainted</code>. However, as long as you correctly annotate the program points, the checker will ensure there is no undesired information flow.
</p>

<p style="font-weight: 200;">
Some example of the <a href="https://checkerframework.org/manual/#tainting-many-uses">purposes</a> that the Tainting Checker can serve is as follows:
Prevent SQL injection attacks: external input is <code>@Tainted</code>, and <code>@Untainted</code> is checked for SQL syntax.
Prevent cross-site scripting attacks: external input is <code>@Tainted</code>, and <code>@Untainted</code> is checked for JavaScript syntax.
Prevent information leak: secret data is <code>@Tainted</code>, and <code>@Untainted</code> may be displayed to the user.
</p>


<strong style="font-weight:500;">Example</strong>
<pre>
void processRequest() {
  @Tainted String input = getUserInput();
  executeQuery(input); // error: pass tainted string to executeQuery
}

public String validateInput(String userInput) {
  // Do some validation here.
  @Untainted String result = userInput;
  return result;
}
</pre>

<p style="font-weight: 200;">
The most important annotations supported by the Tainting Checker are the following:<br>
<ul>
<li><a href="https://checkerframework.org/api/org/checkerframework/checker/tainting/qual/Untainted.html"><code>@Untainted</code></a>: this annotation indicates a type that includes only trusted values.</li>
<li><a href="https://checkerframework.org/api/org/checkerframework/checker/tainting/qual/Tainted.html"><code>@Tainted</code></a>: this annotation indicates a type that may include untrusted values. It is a supertype of <code>@Untainted</code>.</li>
</ul>
</p>

<h2>Checker Framework in Action</h2>
<p style="font-weight: 200;">
Follow the step-by-step instructions to install and use the Checker Framework to find a security error using the Tainting Checker.
</p>

<strong style="font-weight:500;">Step 1: Install</strong>
<p style="font-weight: 200;">First, install Java JDK and ANT build tools.</p>

<pre>
$ apt-get install openjdk-8-jdk
$ apt-get install ant
</pre>

<p style="font-weight: 200;">Then, follow the instructions to install the checker framework:</p>

<pre>
$ wget https://checkerframework.org/checker-framework-3.3.0.zip
$ unzip checker-framework-3.3.0.zip
$ cd checker-framework-3.3.0
$ pwd
</pre>

<p style="font-weight: 200;">Copy the value that is printed to the terminal after running pwd. We will need it later!</p>


<strong style="font-weight:500;">Step 2: Download</strong>
<p style="font-weight: 200;">
Download the source files from the checker framework website, and unzip it. You will see various examples under the <code>src</code> directory. We are going to work with the personal blog demo in this section.
</p>

<pre>
$ wget https://checkerframework.org/tutorial/sourcefiles.zip
$ unzip sourcefiles
$ cd src/personalblog-demo
</pre>

<strong style="font-weight:500;">Step 3: Run the Checker</strong>
<p style="font-weight: 200;">
For building the project, and running the checker at the same time, we use ANT build system to make the process easier. Go ahead and take a look at <code>src/personalblog-demo/build.xml</code>. On the 4th line of this file, change the value to the value you copied in the first step. So it will look like this:</p>

<pre>
&lt;property name=”checker-framework” value=”/checker-framework-3.3.0”/&gt;
</pre>

<p style="font-weight: 200;">
On line 18, the location of the Checker jar file is specified. Then, on line 43, the kind of the checker that we want to use is specified. In this example, we are going to use the Tainting Checker:</p>

<pre>
&lt;compilearg value=”org.checkerframework.checker.tainting.TaintingChecker”/&gt;
</pre>

<p style="font-weight: 200;">
Now, for building this project with ANT, run the following command:</p>

<pre>
$ ant
</pre>

<p style="font-weight: 200;">
The checker will emit an error complaining about an untainted string on line 162 of <code>PersonalBlogService.java</code>:</p>

<pre>
[jsr308.javac]  + "%' order by post.created desc");
[jsr308.javac]    ^
[jsr308.javac]  found   : @Tainted String
[jsr308.javac]  required: @Untainted String

BUILD FAILED
</pre>

<p style="font-weight: 200;">
Now that we have identified where the problem is, we can fix the issue in PersonalBlogService.java by forcing the client to only pass an <code>@Untainted</code> String to the function <code>getpostsByCategory()</code>.</p>

<strong style="font-weight:500;">Step 4: Re-run the Checker</strong>
<p style="font-weight: 200;">Using ANT, re-build the modified project. You will see another error on line 55 of <code>ReadAction.java</code>:
</p>

<pre>
[jsr308.javac]  ...getPostsByCategory(reqCategory));
[jsr308.javac]                        ^
[jsr308.javac]  found   : @Tainted String
[jsr308.javac]  required: @Untainted String

BUILD FAILED
</pre>

<p style="font-weight: 200;">
This time, we are going to correct the tainting error by validating the string. There is already a <code>validate()</code> function in the code that, given any string, validates the string and returns an <code>@Untainted</code> one. Use this function to validate <code>reqCategory</code>.
After fixing the bug, re-build the project. This time, if corrected properly, the checker will not complain and you should expect the following message:
</p>

<pre>
BUILD SUCCESSFUL
</pre>

<hr>
</details>

<details>
<summary><large>A Fuzzing Lesson (AFL)  <small style="font-weight: lighter;"> April 13, 2020 </small></large><br></summary>
<br>
<p style="font-weight: 200;">
Do not be misled by the title; AFL does not stand for <i>A Fuzzing Lesson</i>. It stands for <i>American Fuzzy Lop</i>: one of the cutest animals out there. You might wonder what this rabbit is doing here since this is not a biology class. In fact, the state-of-the-art fuzzer that you are going to learn about is named after this cute rabbit.
</p>

<p style="font-weight: 200;">
<a href="https://lcamtuf.coredump.cx/afl/">AFL</a> is a security-oriented fuzzer written in C that combines compile-time instrumentation with a genetic algorithm to discover crash-inducing inputs. In contrast to other fuzzers, AFL’s focus is practicality: low-performance overhead, very few configuration options, and handling real-world programs.
</p>

<h2> Design Goals of AFL </h2>
<p style="font-weight: 200;">
We start out by discussing the design goals of AFL which make it widely usable.
</p>

<p style="font-weight: 200;">
<strong style="font-weight:500;">1. AFL is Fast.</strong> AFL uses a combination of tricks that make it faster compared to other fuzzers. First, it takes advantage of low-level instrumentation which imposes negligible overhead. Second, it prioritizes mutation strategies that lead to discovering more paths. Finally, it re-evaluates the queue of generated inputs on specific intervals using a fast algorithm that selects a smaller subset of test cases that still cover every exercised tuple of the form <code>(branch_source, branch_destination)</code> so far. Every queue entry is assigned a score proportional to its execution latency and file size. Then, the candidate with the lowest score is selected for each tuple.
</p>

<p style="font-weight: 200;">
<strong style="font-weight:500;">2. AFL Focuses on Code Coverage.</strong> Coverage-guided fuzzing (aka grey-box fuzzing) tries to maximize the code coverage of a program such that more and more code branches are exercised. AFL instrumentation captures branch coverage and hit counts. It maintains an array <code>shared_mem</code> which is a 64 KB region passed to the instrumented binary. It keeps the information regarding the branch source and branch destination for each branch as tuples. So, it distinguishes between the following
execution paths and tries to discover new paths in each iteration. For example, it can distinguish between these execution paths:</p>

<pre>
(1) A --> B --> C --> D
(2) A --> B --> D --> C
</pre>

<p style="font-weight: 200;">since the recorded tuples are different:</p>
<pre>
(1) (A,B) (B,C) (C,D)
(2) (A,B) (B,D) (D,C)
</pre>

<p style="font-weight: 200;">
<strong style="font-weight:500;">3. AFL is Easy to Use.</strong>
AFL is designed to be highly practical. Compared to other fuzzers, AFL requires virtually no configuration and fine-tuning. You can  jump to the end of this article for a quick guide describing how to run AFL. Thereafter, you will be set to start using AFL to hunt bugs in programs!
</p>


<p style="font-weight: 200;">
<strong style="font-weight:500;">4. AFL Can be Chained to Other Tools.</strong>
Although AFL can be used without additional options, one can use additional tools to enhance the effectiveness of AFL and find bugs that might go unnoticed when using the vanilla fuzzer. For instance, <a href="https://en.wikipedia.org/wiki/AddressSanitizer">AddressSanitizer</a> can be added as a compiler option to enable detecting invalid memory accesses.
</p>

<p style="font-weight: 200;">
<strong style="font-weight:500;">5. AFL Can Minimize Generated Inputs.</strong>
AFL provides a tool, <code>afl-tmin</code> and <code>afl-cmin</code>, for test case minimization. It takes an input file and tries to trim as much data as possible while producing the same crashing state or instrumentation output. In other words, it minimizes the input without altering the execution path. This is especially useful when reporting or investigating a crash.
</p>

<h2>Kinds of Input Mutations</h2>

<p style="font-weight: 200;">
AFL is based on a set of mutation strategies that are shown to utilize CPU efficiently and generate interesting test cases. Here are some of <a href="https://lcamtuf.blogspot.com/2014/08/binary-fuzzing-strategies-what-works.html">the main mutation strategies</a> for generating new inputs that make AFL sophisticated:
</p>


<p style="font-weight: 200;">
<strong style="font-weight:500;">1. Bit Flips:</strong> This simple strategy involves sequential, ordered flips for 1-4 bits. The observed results demonstrate 10 to 70 new additional paths per million inputs generated.
</p>

<pre>
<div>               <kiv>
AFL Rocks!   ==>    AFL Rhcks!
</div>              3/div>
</pre>

<p style="font-weight: 200;">
Although the number of the additional paths seems small, since AFL usually explores around 1000 inputs per second per core, such paths appear fairly quickly. For instance, on an 8-core machine, it takes 125 seconds to explore a million inputs. Moreover, discovering one unseen path can lead to discovering more unseen paths, as AFL uses the newly discovered paths as feedback to quickly exercise unseen branches.
</p>

<p style="font-weight: 200;">
<strong style="font-weight:500;">2. Byte Flips:</strong> A natural extension to the previous strategy is flipping bytes. This strategy involves flipping 8-, 16-, or 32-bit wide bitflip. Nearly 30 additional paths are discovered per million generated inputs. Note that this strategy is much cheaper compared to flipping bits due to the underlying hardware support for flipping bytes. However, it limits the opportunities to find additional paths.
</p>

<pre>
<div>               C   >
AFL Rocks!   ==>    AFL Rhcks!
</div>              Pv>
</pre>

<p style="font-weight: 200;">
<strong style="font-weight:500;">3. Simple Arithmetic:</strong> For triggering more complex conditions deterministically, AFL increments or decrements constant integers by 1-35. The effectiveness of this strategy varies depending on the input format. For instance, in JPEG, it helps discover around 2 additional paths per million inputs.
</p>

<p style="font-weight: 200;">
<strong style="font-weight:500;">4. Known Integers:</strong> Another deterministic strategy in AFL involves replacing the numbers in the input file with a hardcoded set of integer numbers that are likely to trigger edge, crashing cases. Examples of such integers include 0, -1, MAX_INT, and MIN_INT-1. This strategy leads to discovering 2-5 additional paths per one million generated inputs.
</p>

<pre>
char *t                       ==>     char *t
= malloc(128*sizeof(char));           = malloc(MAX_INT*sizeof(char));
</pre>

<p style="font-weight: 200;">
<strong style="font-weight:500;">5. Stacked Tweaks:</strong> This strategy randomly stacks some (or all) of the following operations and applies them: single bit-flip and byte-flip, block duplication, block deletion, etc. The number of operations that are stacked is randomly chosen as a power-of-two between 1 and 64, and the block size for block operations is usually 1 KB.
</p>

<p style="font-weight: 200;">
<strong style="font-weight:500;">6. Splicing:</strong> In contrast to previous strategies, this one relies on two distinct inputs. Then, they are spliced in a random location. This strategy sometimes discovers 20% additional paths.
</p>

<h2>Real Bugs Found by AFL</h2>
<p style="font-weight: 200;">
AFL has been able to find <a href="http://lcamtuf.coredump.cx/afl/#bugs">many interesting bugs</a> to date, around 500 of which have been security vulnerabilities, and [have been officially reported](https://github.com/mrash/afl-cve). Here is a sampling of the kinds of bugs together with links to the corresponding vulnerabilities found by AFL:
</p>

<p style="font-weight: 200;">
<ol>
<li>Assertion violation [example: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2016-9399">CVE-2016-9399</a>]
<li>Buffer overflow [example: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-8126">CVE-2015-8126</a>]
<li>Integer overflow [example: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-6839">CVE-2017-6839</a>]
<li>Null dereference [example: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-7475">CVE-2017-7475</a>]
<li>Infinite loop [example: <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-5852">CVE-2017-5852</a>]
</ol>
</p>

<h2>Quick Guide to Run AFL</h2>

<strong style="font-weight:500;">Part 1: Running the Fuzzer</strong>

<p style="font-weight: 200;">
Follow the step-by-step instructions to install and run AFL.
</p>

<p style="font-weight: 200;">
1. Install AFL:
</p>
<pre>
$ apt-get install -y afl
</pre>

<p style="font-weight: 200;">
2. Download fuzzgoat source code, extract it, and build it with afl-clang. This step will inject instrumentations into the target program.
</p>
<pre>
$ git clone https://github.com/fuzzstati0n/fuzzgoat
$ cd fuzzgoat
$ export CC=afl-clang
$ make -j8
</pre>

<p style="font-weight: 200;">
3. Next, create an input folder where the seed file will reside, and an output folder where the generated crashing inputs will go.
</p>
<pre>
$ mkdir input output
</pre>

<p style="font-weight: 200;">
4. We are going to use a valid ELF file which is already on the system as the seed input.
</p>
<pre>
$ cp /bin/ps input/
</pre>

<p style="font-weight: 200;">
5. Finally, run the fuzzer:
</p>
<pre>
$ afl-fuzz -i input -o output -- ./fuzzgoat @@
</pre>

<p style="font-weight: 200;">
You will immediately see the following GUI starting in the terminal. After a few seconds, you should see some crashing inputs. You can find them under the output/crashes folder. You can find more details about the status screen in AFL’s <a href="http://lcamtuf.coredump.cx/afl/status_screen.txt">documentation</a>.
</p>

<strong style="font-weight:500;">Part 2: Minimizing the Crashing Inputs</strong>

<p style="font-weight: 200;">
Next, we are going to try out afl-tmin that performs test minimizing. This tool minimizes the test case to the bare minimum that results in the same execution path. It iterates over the actual bytes in the test case and removes smaller and smaller blocks of bytes. You can observe this tool in action using the following command.
</p>

<pre>
$ afl-tmin -i output/crashes/[crash-file] -o test.min -- ./fuzzgoat
</pre>
<hr>
</details>



</div>
     
<footer class="site-footer" role="banner">
<nav class="site-nav"><small>Icons by </small><a href="https://icons8.com"><img style="width:15px;height:auto" src="./_images/icon8.png"></a> . <small>Original design by</small> <a href="https://andrewhead.info/"><img style="width:20px;height:auto" src="./_images/designby.png"></a></nav>
</footer>

</main>

</body>
</html>
